# Introduction

## How to collect data for the ORBIT dataset project?

Hello and welcome to the ORBIT dataset project! Thank you for joining blind and low vision people across the world to create a large dataset of videos. The first use of this dataset will be to build an app like Seeing AI that uses artificial intelligence, abbreviated to AI, and your phone’s camera to identify things that are important to you, not just generic things. For example, your keys that are no longer where you put them or a friend’s car that is parked somewhere in front of your house.

To contribute, you will be asked to take several short videos of five things that are important to you. To make sure our algorithms ultimately work for blind and low vision people, it is really important that the videos come from you and not from sighted helpers. Before you start recording you videos you should read the [filming instructions](https://orbit.city.ac.uk/phase-2-data-collection/#instructions) which will help you to take good videos. You can expect to spend about 1 hour reading the instructions and then recording the videos.

Participants who record all their videos at once found it easier to complete their contribution. A 10£ donation will be made to a charity of your choice after you submit 5 complete things during our data collection period.

## How does AI work?

Before you begin taking videos, it might help to understand how they will be used. Apps like Seeing AI use Artificial Intelligence, or AI, to automatically recognise the things users record with their phone camera. To do this, these AI algorithms have to be “taught” what to look for by analysing videos of the things you want them to recognise – these are called training videos. AI algorithms learn best when each training video shows each object on its own from different angles and in different locations. This helps the algorithm learn what is important and what to ignore. Once the algorithm has learned to recognise a thing, we need to test its ability to find that thing in a realistic place and in amongst other things, like keys on a bookshelf. We call this a scene. For us to be able to test the AI, we need you to record testing videos in these realistic scenes.    Before you begin taking videos, it might help to understand how they will be used. Apps like Seeing AI use Artificial Intelligence, or AI, to automatically recognise the things users record with their phone camera. To do this, these AI algorithms have to be “taught” what to look for by analysing videos of the things you want them to recognise - these are called training videos. AI algorithms learn best when each training video shows each object on its own from different angles and in different locations. This helps the algorithm learn what is important and what to ignore. Once the algorithm has learned to recognise a thing, we need to test its ability to find that thing in a realistic place and in amongst other things, like keys on a bookshelf. We call this a scene. For us to be able to test the AI, we need you to record testing videos in these realistic scenes.
